---
title: 'Advanced Time Series Analysis: Computer Exercise 2'
author: "Anders Launer Bæk (s160159)"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes: 
    - \usepackage{graphicx}
output:
  pdf_document: default
---

```{r setup, include=FALSE}
rm(list = ls())

knitr::opts_chunk$set(echo = FALSE, 
                      include = TRUE,
                      warning = FALSE,
                      fig.width = 8, fig.height = 4,
                      fig.show='hold', fig.align='center',
                      
                      eval = TRUE, 
                      tidy = TRUE, 
                      dev = 'pdf', 
                      cache = TRUE, fig.pos = "th!")

kable_format <- list(small.mark = ",",
                     big.mark = ',',
                     decimal.mark = '.',
                     nsmall = 3,
                     digits = 3,
                     scientific = FALSE,
                     big.interval = 3L)

library(ggplot2)
theme_TS <- function(base_size = 9, base_family = "", face = "plain"){
  theme_bw(base_size = base_size, base_family = base_family) %+replace%
    theme(panel.background = element_blank(), 
          panel.border = element_blank(),
          panel.grid = element_blank(),
          axis.text = element_text(size = base_size, face = face, family = base_family),
          axis.title = element_text(size = base_size, face = face, family = base_family),
          legend.text = element_text(size = base_size, face = face, family = base_family))
}

```
Sparring partners:
\begin{itemize}
\item Anja Liljedahl Christensen (s162876)
\item Marie Mørk (s112770)
\end{itemize}

## Part 1
There has been simulated $n=3000$ samples of noise where $\epsilon_t \sim \mathcal{N}(0,\,1)$. $\epsilon_t$ is used as noise input for simulations in part one.
```{r}
## Number of samplepoints
n <- 3000
load(file = "~/DTU/Courses/Advanced Time Series/Projects/noise.Rda")

data <- data.frame(t = 1:n, noise = r[1:n])

# define parameter for SETAR
a0 <- c(0.125, -0.125) # off set
a1 <- c(0.6, -0.4) # slope

```
The equation below shows the used parameters in the selected system: a SETAR(2,1,1) model. The parameters for the two regimes are defined by eqn. \ref{eq_1_2a} and eqn. \ref{eq_1_2b}.

\begin{equation}
a_0 = [`r a0[1]`, `r a0[2]`]
\label{eq_1_2a}
\end{equation}
\begin{equation}
a_1 = [`r a1[1]`, `r a1[2]`]
\label{eq_1_2b}
\end{equation}

 
### Simulation of the SETAR(2,1,1)

The SETAR(2,1,1) model is given by eqn. \ref{eq_1_SETAR}.

\begin{equation}
X_t = a_0^{(J_t)} + \sum_{i = 1}^{k_{(J_t)}} a_i^{(J_t)} X_{t-i}+\epsilon^{(J_t)}
\label{eq_1_SETAR}
\end{equation}
where $J_t$ are the regime processes. The complete model are defined in eqn. \ref{eq_1_SETAR_r}.

\begin{equation}
X_t = \left\{ \begin{matrix} a_{0,1} + a_{1,1} X_{t- 1} + \epsilon_t & for & X_{t-1} \leq0 \\  a_{0,2} + a_{1,2} X_{t- 1} + \epsilon_t &  for & X_{t-1} >0 \end{matrix} \right\}
\label{eq_1_SETAR_r}
\end{equation}

A simulation of the model (eqn. \ref{eq_1_SETAR_r}) is showed in fig. \ref{fig_ex_1}. 

```{r, fig.cap="\\label{fig_ex_1}One simulation of a SETAR(2,1,1) model."}
## Make a time series y with a regime model
data$y_SETAR <- rep(NA,n)
data$y_SETAR[1] <- data$noise[1]

for(t in 2:n) {
  if(data$y_SETAR[t - 1] <= 0) {
    data$y_SETAR[t] <- a0[1] + a1[1] * data$y_SETAR[t-1] + data$noise[t]
  } else {
    data$y_SETAR[t] <- a0[2] + a1[2] * data$y_SETAR[t-1] + data$noise[t]
  }
}

#
data$y_SETAR_1 <- rep(NA, n)
data$y_SETAR_1[2:n] <- data$y_SETAR[1:(n-1)]


data$mean_theo <- rep(NA, n)

# plot data
ggplot(data) +
  geom_point(aes(x = y_SETAR_1, y = y_SETAR, color = "SETAR(2,1,1)"), alpha = 1/2) +
  #ylim(0, 1) +
  labs(x = "y(t - 1)", y = "y(t)", color = "") +
  theme_TS()
```

### Estimate the parameters using conditional least squares
The following code snippet contain three functions: `Setar()`, `RSSSetar` and `PESetar` calculates the conditional means, the squared residuals and the total squared prediction error respectively. 
```{r, echo=TRUE}
# calculation of the conditional mean for the SETAR(2,1,1) model
Setar <- function(par, model) {
  # init conditional mean vector
  e_mean <- rep(NA, length(model))
  # loop through observations
  for (t in 2:length(model)) {
    if (model[t - 1 ] <= 0) {
      e_mean[t] <- par[1] + par[2] * model[t - 1]
      } else {
        e_mean[t] <- par[3] + par[4] * model[t - 1]
      }
    }
  # return the conditional mean vector
  return(e_mean)
}

RSSSetar <- function(par, model) {
  # conditional mean
  e_mean <- Setar(par, model)
  # calculate and return the squared residuals
  return((model - e_mean)^2)
}

# summed prediction error
PESetar <- function(par, model) {
  # conditional mean
  e_mean <- Setar(par, model)
  # calculate and return the objective function value
  return(sum((model - e_mean)^2, na.rm = TRUE))
}
```

The native R function `optim()` has been applied to estimate the parameters of the simulated process. The objective cost function is the total squared prediction error, which needs to be minimized. 
```{r}
a0_new <- c(0.1, -0.02)
a1_new <- c(0.4,-0.25)
```
eqn. \ref{eq_1_2a_new} and eqn. \ref{eq_1_2b_new} have been used as initial parameter input to optimization function. The "initial" conditional mean is computed with the initial parameters (eqn. \ref{eq_1_2a_new} and eqn. \ref{eq_1_2b_new}) and illustrated in fig. \ref{fig_ex_2} with the label: `M(x) initial`.

\begin{equation}
a_0 = [`r a0_new[1]`, `r a0_new[2]`]
\label{eq_1_2a_new}
\end{equation}
\begin{equation}
a_1 = [`r a1_new[1]`, `r a1_new[2]`]
\label{eq_1_2b_new}
\end{equation}

The estimated parameters are listed in table \ref{tb_ex_1}.

```{r}
# optimize
optimal_PE <- optim(par = c(a0[1],a1[1],a0[2],a1[2]), fn = PESetar, model = data$y_SETAR)
# 
data$y_SETAR_opti <- Setar(par = optimal_PE$par, model = data$y_SETAR)
data$y_SETAR_mean <- Setar(par = c(a0[1],a1[1],a0[2],a1[2]), model = data$y_SETAR)
data$y_SETAR_before <- Setar(par = c(0.1,0.4,-0.02,-0.25), model = data$y_SETAR)
data$rsssetar <- RSSSetar(par = optimal_PE$par, model = data$y_SETAR)

# table ----
df_opti <- data.frame(par_name = c("a0_1","a1_1","a0_2","a1_2"), init_par = c(a0[1],a1[1],a0[2],a1[2]), opti_par = optimal_PE$par)
df_opti$d_par <- (df_opti$opti_par - df_opti$init_par) / df_opti$opti_par * 100
colnames(df_opti) <- c("Parameter", "Real value", "Optimized value", "Change in (%)")
knitr::kable(df_opti, 
             #format.args = kable_format, 
             format = "latex", 
             caption = "\\label{tb_ex_1}Table shows the real parameters, the estimated parameters and their percentage diviation.", 
             booktabs = TRUE)
```

The percentage deviation from the real parameters are within $\pm `r max(abs(df_opti[, 4]))`\%$. The optimized squared prediction error (`r PESetar(par = optimal_PE$par, model = data$y_SETAR)`) is smaller than the real squared error (`r PESetar(par = c(a0[1],a1[1],a0[2],a1[2]), model = data$y_SETAR)`) which is a subject of overfitting. Dividing data into train and test with proper cross validation techniques can solve the issue of overfitting.

Figure \ref{fig_ex_2} illustrate the conditional means for the SETAR(2,1,1) model with initial parameters (eqn. \ref{eq_1_2a_new} and eqn. \ref{eq_1_2b_new}), the real and estimated parameters (table \ref{tb_ex_1}). 

```{r, fig.cap="\\label{fig_ex_2}Simulations of a SETAR(2,1,1) model with the initial, real and optimized parameters. The vertical lines in the transition of the regimes does not exist."}
# plot data
ggplot(data) +
  geom_point(aes(x = y_SETAR_1, y = y_SETAR, color = "SETAR(2,1,1)"), alpha = 1/2) +
  geom_line(aes(x = y_SETAR_1, y = y_SETAR_before, color = "M(x) initial"), alpha = 1/2) +
  geom_line(aes(x = y_SETAR_1, y = y_SETAR_mean, color = "M(x) real"), alpha = 1/2) +
  geom_line(aes(x = y_SETAR_1, y = y_SETAR_opti, color = "M(x) optimized"), alpha = 1/2) +
  #ylim(0, 1) +
  labs(x = "y(t - 1)", y = "y(t)", color = "") +
  theme_TS()
```

The initial parameters are not a good representation of the simulated SETAR model, as illustrated in fig. \ref{fig_ex_2} with the label `M(x) initial`. But after estimating the parameters s.t. a minimization of the squared prediction error, the estimated parameters is a reasonable representation of the SETAR model.

\newpage
## Part 2
```{r}
max_change_p <- 0.3
resolution <- 51 # must be odd
```

It has been chosen to create a grid based upon the two slope parameters, eqn. \ref{eq_1_2b}. The grid is based on a matrix with the size $(`r resolution`, `r resolution`)$, where the centre element, the intersection of the diagonal and off-diagonal, is the optimal parameter value, and is highlighted with a blue dot. There is also illustrated a red dot, which shows the optimal parameters for the given subset of the simulation. 
The first axis is $a_{1,1}$ parameter and the second axis in the $a_{1,2}$ parameter.

The the deviation of the parameter values are $\pm`r max_change_p * 100`\%$ (`max_change_p`) of the optimal value. The following function has been used to plot the contours for the different subsets: `plot_contours()`. General to the contour plots, close contour lines identicate a higher slope of the curvature of the objective function. 

```{r, echo=TRUE}
# only change the slope par[2] and par [4]
plot_contours <- function(model, par = optimal_PE$par, change_p = max_change_p, nplot = resolution){ 
  # create squence of nplot values with the optimized parameter(s) in center
  par_2_seq <- seq(par[2] - par[2] * change_p, par[2] + par[2] * change_p, len = nplot)
  par_4_seq <- seq(par[4] - par[4] * change_p, par[4] + par[4] * change_p, len = nplot)
  
  # create grid
  loess_melt <- expand.grid(par_2_seq, par_4_seq)
  
  # caculate values
  loess_melt$value <- sapply(1:nrow(loess_melt), function(i) { 
    return(PESetar(par = c(par[1], loess_melt$Var1[i],
                           par[3], loess_melt$Var2[i]),
                   model = model)) }) 
  
  # find optimal subset value 
  min_val <- loess_melt[which.min(loess_melt$value), ]
  
  # return contour plot 
  return(ggplot(loess_melt, aes(x = Var1, y = Var2, z = value)) +
          stat_contour(geom="contour", alpha = 1/2, binwidth = 0.005) +
          geom_point(aes(x = par[2], y = par[4], 
                         color = paste0("optim,\nval=",
                                        round(PESetar(par = par,
                                                      model = model), 
                                              digits = 4))), 
                     alpha = 1/2) +
          geom_point(aes(x = min_val$Var1, y = min_val$Var2, 
                         color = paste0("best subset,\nval=",
                                        round(min_val$value, digits = 4))), 
                     alpha = 1/2) +
          labs(x = "a_1,1", y = "a_1,2", color = "parameter and value") +
          theme_TS())
}
```

The contour plots illustrates the objective function (total squared prediction error) with changes in each changeable parameter. The objective function is convex which means there is only one local/global minima.

### N = 1:3000
Figure \ref{fig_ex_2_1} shows the contour plot for the complete simulated model. The optimal parameters gives the global minima of the objective function, and the two dots are identical as illustrated. 

The $a_{1,1}$ parameter will result in a higher objective function with equally changes in both parameter.
```{r, fig.cap="\\label{fig_ex_2_1}Contour plot the slope parameters for N = 1:3000."}
N = 1:3000
plot_contours(model = data$y_SETAR[N], par = optimal_PE$par, change_p = max_change_p, nplot = resolution)
```

### N = 1:300
Figure \ref{fig_ex_2_2} shows the contour plot with only $N = 1:300$ samples of the simulated model considered. The number of samples are decreased with a factor of 100 similar to the objective function.

The optimal parameters does not represent the minimum value of the of the objective function. There is a clear separation of the two dots and the their values are different, see labels in fig. \ref{fig_ex_2_2}.

```{r, fig.cap="\\label{fig_ex_2_2}Contour plot the slope parameters for N = 1:300."}
N = 1:300
plot_contours(model = data$y_SETAR[N], par = optimal_PE$par, change_p = max_change_p, nplot = resolution)
```

### N = 1:30
Figure \ref{fig_ex_2_3} shows a contour plot of the first 30 samples. It is possible to see that the best set of parameter, in the given subset, is not within $\pm `r max(abs(df_opti[, 4]))`\%$ of the optimized parameters. In contrast to fig. \ref{fig_ex_2_3} there red dot is placed on the boundary of the grid which indicates that the global minimum is not obtained.

```{r, fig.cap="\\label{fig_ex_2_3}Contour plot the slope parameters for N = 1:30."}
N = 1:30
plot_contours(model = data$y_SETAR[N], par = optimal_PE$par, change_p = max_change_p, nplot = resolution)
```

### N = 1001:1300
Figure \ref{fig_ex_2_4} uses same number of simulated samples but in an different "location" of the simulated data. The optimal subset parameters is not within $\pm `r max_change_p * 100`\%$ of the optimized parameters which is opposite to fig. \ref{fig_ex_2_2}.
```{r, fig.cap="\\label{fig_ex_2_4}Contour plot the slope parameters for N = 1001:1300."}
N = 1001:1300
plot_contours(model = data$y_SETAR[N], par = optimal_PE$par, change_p = max_change_p, nplot = resolution)
```

### N = 1001:1030
The best subset parameters in fig. \ref{fig_ex_2_5} are not within $\pm `r max_change_p * 100`\%$ of the optimized parameters.

```{r, fig.cap="\\label{fig_ex_2_5}Contour plot the slope parameters for N = 1001:1030."}
N = 1001:1030
plot_contours(model = data$y_SETAR[N], par = optimal_PE$par, change_p = max_change_p, nplot = resolution)
```

### Findings
\begin{itemize}
\item The optimization of the parameters must occur with respect to the wanted (sub)set of the simulated data. 
\item The biggest changes for the optimal subset parameters is for the $a_{1,2}$ parameter. In figures \ref{fig_ex_2_2} and \ref{fig_ex_2_3} the best subset $a_{1,2}$ parameter is smaller then the optimized value of $a_{1,2}$.\\ 
Opposite in figures \ref{fig_ex_2_4} and \ref{fig_ex_2_5}, the best subset $a_{1,2}$ parameter must be higher than the optimized $a_{1,2}$ parameter. 
\item The total squared prediction error scales well with the number of considered simulated data. This can identicate that the residuals are normally distributed over the entire simulated process. 
\end{itemize}


\newpage
## Part 3
The chosen model for the doubly stochastic model, is an AR(2)-AR(4) model, see eqn. \ref{eq_3_1}.

\begin{equation}
\begin{aligned}
Y_t &= \sum_{k = 1}^2 \left( \Phi_{t-(1-k)} Y_{t-k} \right) +\epsilon_t \\
\Phi_t - \mu &= \sum_{n = 1}^4\left( \phi_{n}\left( \Phi_{t-n}- \mu\right)  \right) + \zeta_t \\
\Phi_{ t } &= \sum_{ n=1 }^{ 4 } \left( \phi_{ n }\left( \Phi_{ t-n }-\mu  \right)  \right) +\zeta_{ t }+\underset { \delta_t }{ \underbrace { \mu \left( 1-\sum_{ n=1 }^{ 4 } \left( \phi_{ n } \right)  \right)  }  } 
\end{aligned}
\label{eq_3_1}
\end{equation}

Equation \ref{eq_3_2} shows eqn. \ref{eq_3_1} in a reparametrized state space format.

\begin{equation}
\begin{aligned}
\begin{pmatrix}  \Phi_{t} \\ \Phi_{t-1}\\ \Phi_{t-2}\\ \Phi_{t-3} \\ \delta_t  \end{pmatrix} &=\begin{pmatrix} \phi_1 & \phi_2  & \phi_3  & \phi_4 &1 \\ 1 &0&0&0&0 \\ 0&1&0&0&0 \\ 0&0& 1 &0&0 \\  0&0&0&0&1  \end{pmatrix} \begin{pmatrix}  \Phi_{t-1} \\ \Phi_{t-2}\\ \Phi_{t-3}\\ \Phi_{t-4} \\ \delta_{t-1}  \end{pmatrix} + \begin{pmatrix}  1  \\ 0\\ 0\\ 0\\ 0 \end{pmatrix}\delta_t \\
Y_t &=\begin{pmatrix} Y_{t-1} & Y_{t-2}  & 0& 0& 0 \end{pmatrix}\begin{pmatrix} \Phi_{ t } \\ \Phi_{ t-1 } \\ \Phi_{ t-2 } \\ \Phi_{ t-3 } \\ \delta _{ t } \end{pmatrix}+e_t
\end{aligned}
\label{eq_3_2}
\end{equation}

### Simulate 

```{r}
nn = 500
data_3 <- data.frame(t = 1:nn)

data_3$Y <- rep(NA, nn)
data_3$Phi <- rep(NA, nn)
data_3$delta <- rep(NA, nn)

mu <- 0.1
sigma_2_zeta <- 0.080^2
sigma_2_epsilon <- 0.40^2
set.seed(22)
data_3$zeta <- rnorm(nn, mean = mu, sd = sqrt(sigma_2_zeta))
data_3$epsilon <- rnorm(nn, mean = mu, sd = sqrt(sigma_2_epsilon))
```


The initial parameters for the simulation are given in eqn. \ref{eq_3_4}.
\begin{equation}
\begin{aligned}
n =& `r nn` \\
\mu =& `r mu` \\
\zeta =& `r sqrt(sigma_2_zeta)` \\
\epsilon =& `r sqrt(sigma_2_epsilon)` \\
\delta_{t} \sim& \mathcal{N}( \mu,\,\zeta)  \\
e_{t} \sim& \mathcal{N}( \mu,\,\epsilon)
\end{aligned}
\label{eq_3_4}
\end{equation}

Figure \ref{fig_ex_3_1} and figure \ref{fig_ex_3_2} shows the process and the underlying process respectively. 
```{r, fig.cap="\\label{fig_ex_3_1}Simulated process of Y(t)."}
# init ----
data_3$Y[1:4] <- data_3$epsilon[1:4]
data_3$Phi[1:4] <- data_3$zeta[1:4]
phi <- c(0.85,0.85,0.85,0.85) / 100
data_3$delta[1] <- mu * (1 - sum(phi))

# underlying ----
A <- matrix(data = c(phi[1],phi[2],phi[3],phi[4],1,
                1,0,0,0,0,
                0,1,0,0,0,
                0,0,1,0,0,
                0,0,0,0,1), nrow = 5, ncol = 5, byrow = T)


X <- matrix(data = c(data_3$Phi[nrow(A) - 1],
                     data_3$Phi[nrow(A) - 2],
                     data_3$Phi[nrow(A) - 3],
                     data_3$Phi[nrow(A) - 4],
                     data_3$delta[1]), nrow = 5)


ZETA <- matrix(data = c(1,0,0,0,0), nrow = 5)
for (i in nrow(A):nn) {
  # underlying   
  X <- A %*% X + ZETA * data_3$zeta[i]  
  # capture Phi
  data_3$Phi[i] <- X[1]
  data_3$delta[i] <- X[5]
  # overlying   
  data_3$Y[i] <- matrix(data = c(data_3$Y[i - 1],
                                 data_3$Y[i - 2],
                                 0,0,0), ncol = 5) %*% X + data_3$epsilon[i]
}

ggplot(data_3) +
  geom_line(aes(x = t, y = Y, color = "Y(t)"), alpha = 1/2) +
  labs(x = "t", y = "Y(t)", color = "") +
  theme_TS()
```

```{r, fig.cap="\\label{fig_ex_3_2}Simulated process of Phi(t)."}
ggplot(data_3) +
  geom_line(aes(x = t, y = Phi, color = "Phi(t)"), alpha = 1/2) +
  labs(x = "t", y = "Phi(t)", color = "") +
  theme_TS() 



```



### Comment
\begin{itemize}
\item Figure \ref{fig_ex_3_2} shows the underlying process of the doubly stochastic model and the its shows a rapidly increase after the first samples where the process more less stabilize around $`r round(mean(data_3[,"Phi"]), 2)`$ afterwards.
\item Choosing the initial parameters for this doubly stochastic model is critical and needs to be done with stability in mind. 
\item todo: Stability in the underlying process? and stability in the overlying process?
\end{itemize}

\newpage
\newpage
## Part 4

Following state space model is given, eqn. \ref{eq_4_1}.
\begin{equation}
\begin{aligned}
x_{t+1} &= a x_t + v_t \\
y_t &= x_t + e_t
\end{aligned}
\label{eq_4_1}
\end{equation}

where $a$ is an unknown parameter and $v_t$ and $e_t$ are mutually uncorrelated white noise processes with their variances $\sigma^2_v$ and $\sigma^2_e$.

### Part 4a

The model from eqn. \ref{eq_4_1} is on state space form in eqn. \ref{eq_4_2}
\begin{equation}
\begin{aligned}
x_{t+1} &= a x_t + v_t \\
y_t &= x_t + e_t
\end{aligned}
\label{eq_4_2}
\end{equation}

#### Simulate


```{r}
nnn <- 10000
n_sim <- 20
set.seed(22)
simulate <- function(a = 0.4, sigma2_v = 1, sigma_e = 1, n_sim = n_sim, n = n){
  data <- data.frame(matrix(NA, nrow = n, ncol = n_sim))
  
  data$t <- 1:n
  data$x <- rep(0, n)
  
  for (jj in 1:n_sim){
    
    set.seed(jj)
    sig_v <- rnorm(n, mean = 0, sd = sigma2_v) 
    set.seed(jj + n_sim)
    sig_e <- rnorm(n, mean = 0, sd = sigma_e) 
    
    for (ii in 1:n){
      if (ii < n) {
        data$x[ii + 1] <- a * data$x[ii] + sig_v[ii]  
      }
      data[ii, jj] <- data$x[ii] + sig_e[ii]
    }
  }
  return(data)
}
data_4 <- simulate(a = 0.4, sigma2_v = 1, sigma_e = 1,n_sim = n_sim, n = nnn)

# ggplot(data_4) +
#   geom_line(aes(x = t, y = X1, color = "X1"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X2, color = "X2"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X3, color = "X3"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X4, color = "X4"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X5, color = "X5"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X6, color = "X6"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X7, color = "X7"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X8, color = "X8"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X9, color = "X9"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X10, color = "X10"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X11, color = "X11"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X12, color = "X12"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X13, color = "X13"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X14, color = "X14"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X15, color = "X15"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X16, color = "X16"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X17, color = "X17"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X18, color = "X18"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X19, color = "X19"), alpha = 1/10) +
#   geom_line(aes(x = t, y = X20, color = "X20"), alpha = 1/10) +
#   labs(x = "t", y = "Y(t)", color = "") +
#   theme_TS()
```

There has been simulated `r n_sim` time series with the initial parameters given in eqn. \ref{eq_4_21}.

\begin{equation}
\begin{aligned}
a &= 0.4 \\
\sigma^2_v &= 1 \\
\sigma^2_e &= 1 \\
v_{t} &\sim \mathcal{N}( 0,\,\sigma^2_v) \\
e_{t} &\sim \mathcal{N}( 0,\,\sigma^2_e)
\end{aligned}
\label{eq_4_21}
\end{equation}


#### Rewrite 
Equation \ref{eq_4_2} has been reparametrized to state space form in eqn. \ref{eq_4_3}.
\begin{equation}
\begin{aligned}
\begin{pmatrix}  x_{t+1}  \\  a_{t+1}  \end{pmatrix} &=\begin{pmatrix}  a_t& 0 \\ 0 & a_t \end{pmatrix}\begin{pmatrix}  x_t\\1    \end{pmatrix} +\begin{pmatrix}  v_t \\ 0 \end{pmatrix} \\
y_t &= \begin{pmatrix}  1&0  \end{pmatrix} \begin{pmatrix}  x_t\\1  \end{pmatrix} +e_t
\end{aligned}
\label{eq_4_3}
\end{equation}
$a$ has been included in the state space equation, which makes it possible to estimate the parameter using the filter.

### Part 4b
The following function (`ext_kalman()`) has been used to estimate $a$ for the given sets of initial parameters.
```{r, echo=TRUE}
##----------------------------------------------------------------
## EKF algorithm for use in Part 4 of computer exercise 2 in
## Advanced Time Series Analysis
##----------------------------------------------------------------

ext_kalman <- function(y, aInit = 0.5, aVarInit = 1, sigma.v = 1) {
  ## aInit : The starting guess of the AR coefficient estimate
  ## aVarInit : The initial variance for estimation of the AR coefficient
  ## sigma.v : Standard deviation of the system noise of x in the filter

  # Initialize----
  # Init the state vector estimate
  zt <- c(0,aInit)
  # Init the variance matrices
  Rv <- matrix(c(sigma.v^2,0,0,0), ncol=2)
  # sigma.e : Standard deviation of the measurement noise in the filter
  Re <- 1 

  # Init the P matrix, that is the estimate of the state variance
  Pt <- matrix(c(Re,0,0,aVarInit), nrow=2, ncol=2)
  # The state is [X a] so the differentiated observation function is
  Ht <- t(c(1,0))
  # Init a vector for keeping the parameter a variance estimates
  aVar <- rep(NA,length(y))
  # and keeping the states
  Z <- matrix(NA, nrow=length(y), ncol=2)
  Z[1,] <- zt

  ## The Kalman filtering----
  for(t in 1:(length(y)-1)) {
    # Derivatives (Jacobians)
    Ft <- matrix(c(zt[2],0,zt[1],1), ncol=2)  # F_t-1
    # Ht does not change 
    
    ## Prediction step
    zt = c(zt[2]*zt[1],zt[2]) #z_t|t-1 f(z_t-1|t-1)
    Pt = Ft %*% Pt %*% t(Ft) + Rv #P_t|t-1
    
    ## Update step
    res = y[t] - zt[1] # the residual at time t
    St =  Ht %*% Pt %*% t(Ht) + Re # innovation covariance
    Kt = Pt %*% t(Ht) %*% St^-1 # Kalman gain
    zt = zt + Kt * res # z_t|t
    Pt = (diag(2) - Kt%*%Ht)%*%Pt #P_t|t
    
    ## Keep the state estimate
    Z[t+1,] <- zt
    ## Keep the P[2,2], which is the variance of the estimate of a
    aVar[t+1] <- Pt[2,2]
  }
  return(list("zt" = zt, "Pt" = Pt, "Rv" = Rv, "aVar" = aVar, "Z" = Z))
}

```
There has been executed  a small test to find the number of needed samples for converges of the filter estimate. The test is based upon the worst case scenario where the initial variance of the filter and parameters is $10$. The initial value of the parameter is $a = 0.5$.

Figure \ref{fig_ex_4_1} shows the converges test for the filter estimate. The variance of the parameter and the estimated value of the parameter. 

```{r, fig.cap="\\label{fig_ex_4_1}Converges test of the filter estimate."}
ek <- ext_kalman(data_4$X1, aInit = 0.5, aVarInit = 10, sigma.v = 10)

df <- data.frame(a = ek$Z[,2], a_var = ek$aVar)
df$t <- 1:nrow(df)

ggplot(df) +
  geom_line(aes(x = t, y = a, color = "a"), alpha = 1/2) +
  geom_line(aes(x = t, y = a_var, color = "a_var"), alpha = 1/2) +
  labs(x = "t", y = "", color = "") +
  theme_TS()

nnn_con <- 3000
data_4 <- subset(data_4, subset = 1:n %in% 1:nnn_con)
n <- nrow(data_4)

```
The filter is converged after the first `r nnn_con` samples, (figure \ref{fig_ex_4_1}) and will therefore only consider those first samples.




```{r}


list <- list("a0.5" = list("state1" = list("a_v_init" = 1,
                                           "sigma_v_init" = sqrt(10)),
                           "state2" = list("a_v_init" = 1,
                                           "sigma_v_init" = sqrt(1)),
                           "state3" = list("a_v_init" = 10,
                                           "sigma_v_init" = sqrt(10)),
                           "state4" = list("a_v_init" = 10,
                                           "sigma_v_init" = sqrt(1))),
             "a-0.5" = list("state1" = list("a_v_init" = 1,
                                           "sigma_v_init" = sqrt(10)),
                            "state2" = list("a_v_init" = 1,
                                           "sigma_v_init" = sqrt(1)),
                            "state3" = list("a_v_init" = 10,
                                           "sigma_v_init" = sqrt(10)),
                            "state4" = list("a_v_init" = 10,
                                           "sigma_v_init" = sqrt(1))))
# loop scenarios
# init
a_init <- 0.5
for (scenario in 1:2) {
  # 
  if (scenario == 2) {
    a_init <- a_init * - 1.0
  }
  
  
  for (state in 1:4) {
    list[[scenario]][[state]]$a_estimates <- rep(NA, n_sim)
    list[[scenario]][[state]]$sigma_v <- rep(NA, n_sim)
    for (ii in 1:n_sim) {
      ek <- ext_kalman(data_4[,ii], aInit = a_init, 
                       aVarInit = list[[scenario]][[state]]$a_v_init, 
                       sigma.v = list[[scenario]][[state]]$sigma_v_init)
      # capture estimated a and sigma v
      list[[scenario]][[state]]$a_estimates[ii] <- ek$zt[2]
      list[[scenario]][[state]]$sigma_a[ii] <- ek$Pt[4]
    
    }
    
    list[[scenario]][[state]]$plot_a <- ggplot() +
        aes(list[[scenario]][[state]]$a_estimates) +
        geom_histogram() +
        labs(y = "frequency", x = "a", color = "") +
        theme_TS()
    
    list[[scenario]][[state]]$plot_sigma_a <- ggplot() +
        aes(list[[scenario]][[state]]$sigma_a) +
        geom_histogram() +
        labs(y = "frequency", x = "sigma_v", color = "") +
        theme_TS()
    }
}

desc_stats <- function(scenario) {
  df <- data.frame(matrix(NA, nrow = 4, ncol = 7))
  for (state in 1:4) {
    df[state, 1] <- state
    df[state, 2] <- list[[scenario]][[state]]$sigma_v_init^2
    df[state, 3] <- list[[scenario]][[state]]$a_v_init
    
    df[state, 4] <- mean(list[[scenario]][[state]]$a_estimates)
    df[state, 5] <- sd(list[[scenario]][[state]]$a_estimates)
  
    df[state, 6] <- mean(list[[scenario]][[state]]$sigma_a)
    df[state, 7] <- sd(list[[scenario]][[state]]$sigma_a)
  }
  colnames(df) <- c("combination", "sigma_v^2", "sigma_a", "a mean", "a_var", "a_var mean", "a_var sd")
  return(df)
}

```

Table \ref{tb_4_1} and table \ref{tb_4_2} shows the initial variance values for each combination averages over the `r n_sim` simulations: The mean estimated value of $a$, the standard deviation of the estimated $a$, the mean value of the variance and the standard deviation of the variance.   

#### a = 0.5
Table \ref{tb_4_1} shows the statistics for $a = 0.5$.

```{r}
knitr::kable(desc_stats(scenario = 1), caption = "\\label{tb_4_1}Statistic for filter estimates.")
```


#### a = -0.5
Table \ref{tb_4_2} shows the statistics for $a = -0.5$.
```{r}
knitr::kable(desc_stats(scenario = 2), caption = "\\label{tb_4_2}Statistic for filter estimates.")
```

#### Comments
According to table \ref{tb_4_1} and table \ref{tb_4_2} there is not much of a difference in when selecting different initial parameter value of $a$. Therefore some common comments.
\begin{itemize}
\item The most important parameter, when using filter to estimate parameter(s), is the variance of the filter. The combinations with a low filter variance result in a reasonable parameter estimate, close to the original value of $a$, regarding the initial value of variance of the parameter.
\item In both combinations where $\sigma_v^2 = 1$ is the estimated value of $a$ slightly above the original value of $a$. 
\end{itemize}


### Improvements
Estimate a parameter with an extended Kalman filter will introduce a bias estimate coursed by the properties of the the filter. 
\begin{itemize}
\item The local linearisation, which introduces the bias, can be affected and improved by adding proper scaled Gaussian noise to the estimated parameter in the state space model (eqn. \ref{eq_4_3}) with an decay of $\frac{1}{t}$.
\item By substituting the extended Kalman filter whit an unscented Kalman filter will possibly increase accuracy of the estimated parameter value. The unscented Kalman filter approximates a Gaussian distribution with a deterministic sampling approach and here by captures the mean and covariance, which in most cases outperform the extended Kalman filter\footnote{Mentioned at the lecture October 25.}.
\item An different approach could be to use maximum likelihood estimation of the parameter instead. 
\end{itemize}
